import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
import logging
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
import joblib
import os
import json

from ..models.schemas import (
    SafetyAnalysisRequest, 
    SafetyAnalysisResponse,
    ModelStatus,
    PerformanceMetrics
)

logger = logging.getLogger(__name__)

class SafetyAnalyzer:
    def __init__(self):
        self.model = None
        self.scaler = None
        self.model_version = "1.0.0"
        self.is_initialized = False
        self.performance_metrics = {
            "total_predictions": 0,
            "response_times": [],
            "accuracy_scores": []
        }
        
    async def initialize(self):
        """AI ë¶„ì„ ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        try:
            logger.info("ğŸ§  AI ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘...")
            
            # ëª¨ë¸ ë¡œë“œ ì‹œë„
            model_path = "models/safety_model.joblib"
            scaler_path = "models/safety_scaler.joblib"
            
            if os.path.exists(model_path) and os.path.exists(scaler_path):
                self.model = joblib.load(model_path)
                self.scaler = joblib.load(scaler_path)
                logger.info("âœ… ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            else:
                # ê¸°ë³¸ ëª¨ë¸ ìƒì„±
                await self._create_default_model()
                logger.info("âœ… ê¸°ë³¸ ëª¨ë¸ ìƒì„± ì™„ë£Œ")
            
            self.is_initialized = True
            logger.info("ğŸ§  AI ë¶„ì„ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ AI ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")
            # í´ë°±: ê·œì¹™ ê¸°ë°˜ ë¶„ì„ë§Œ ì‚¬ìš©
            self.is_initialized = True

    async def _create_default_model(self):
        """ê¸°ë³¸ ëª¨ë¸ ìƒì„±"""
        try:
            # ìƒ˜í”Œ ë°ì´í„° ìƒì„± (ì‹¤ì œë¡œëŠ” ì¶•ì ëœ ë°ì´í„° ì‚¬ìš©)
            X_sample, y_sample = self._generate_sample_data()
            
            # ëª¨ë¸ í›ˆë ¨
            self.scaler = StandardScaler()
            X_scaled = self.scaler.fit_transform(X_sample)
            
            self.model = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                random_state=42
            )
            self.model.fit(X_scaled, y_sample)
            
            # ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs("models", exist_ok=True)
            
            # ëª¨ë¸ ì €ì¥
            joblib.dump(self.model, "models/safety_model.joblib")
            joblib.dump(self.scaler, "models/safety_scaler.joblib")
            
            logger.info("âœ… ê¸°ë³¸ ëª¨ë¸ ì €ì¥ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ê¸°ë³¸ ëª¨ë¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")

    def _generate_sample_data(self):
        """ìƒ˜í”Œ í›ˆë ¨ ë°ì´í„° ìƒì„±"""
        np.random.seed(42)
        n_samples = 1000
        
        # íŠ¹ì„± ìƒì„±
        screen_time = np.random.normal(180, 60, n_samples)  # í‰ê·  3ì‹œê°„
        app_open_count = np.random.poisson(20, n_samples)
        hours_since_checkin = np.random.exponential(12, n_samples)
        location_changes = np.random.poisson(5, n_samples)
        
        X = np.column_stack([
            screen_time,
            app_open_count, 
            hours_since_checkin,
            location_changes
        ])
        
        # ë ˆì´ë¸” ìƒì„± (ìœ„í—˜ë„ ê¸°ë°˜)
        risk_scores = (
            (screen_time < 60) * 2 +  # ë‚®ì€ í™”ë©´ ì‹œê°„
            (app_open_count < 5) * 2 +  # ë‚®ì€ ì•± ì‚¬ìš©
            (hours_since_checkin > 24) * 3 +  # ì˜¤ëœ ì²´í¬ì¸ ì—†ìŒ
            (location_changes == 0) * 1  # ìœ„ì¹˜ ë³€í™” ì—†ìŒ
        )
        
        # 0: ì•ˆì „, 1: ì£¼ì˜, 2: ìœ„í—˜
        y = np.where(risk_scores < 2, 0, np.where(risk_scores < 5, 1, 2))
        
        return X, y

    async def analyze_safety_data(self, request: SafetyAnalysisRequest) -> SafetyAnalysisResponse:
        """ì•ˆì „ ë°ì´í„° ë¶„ì„"""
        start_time = datetime.now()
        
        try:
            logger.info(f"ğŸ” ì‚¬ìš©ì {request.user_id} ì•ˆì „ ë¶„ì„ ì‹œì‘")
            
            # íŠ¹ì„± ì¶”ì¶œ
            features = self._extract_features(request)
            
            # AI ëª¨ë¸ ì˜ˆì¸¡ (ëª¨ë¸ì´ ìˆëŠ” ê²½ìš°)
            if self.model is not None and self.scaler is not None:
                risk_level, confidence = await self._predict_with_model(features)
                analysis_method = "AI ëª¨ë¸"
            else:
                # í´ë°±: ê·œì¹™ ê¸°ë°˜ ë¶„ì„
                risk_level, confidence = self._rule_based_analysis(features)
                analysis_method = "ê·œì¹™ ê¸°ë°˜"
            
            # ì¶”ì²œì‚¬í•­ ë° ìœ„í—˜ìš”ì†Œ ìƒì„±
            recommendations = self._generate_recommendations(risk_level, features)
            risk_factors = self._identify_risk_factors(features)
            
            # ì‘ë‹µ ì‹œê°„ ê¸°ë¡
            response_time = (datetime.now() - start_time).total_seconds()
            self.performance_metrics["response_times"].append(response_time)
            self.performance_metrics["total_predictions"] += 1
            
            result = SafetyAnalysisResponse(
                risk_level=int(risk_level),
                confidence=float(confidence),
                recommendations=recommendations,
                risk_factors=risk_factors,
                timestamp=datetime.now().isoformat(),
                model_version=self.model_version,
                analysis_details={
                    "method": analysis_method,
                    "features": features,
                    "response_time_ms": int(response_time * 1000)
                }
            )
            
            logger.info(f"âœ… ì•ˆì „ ë¶„ì„ ì™„ë£Œ: ìœ„í—˜ë„ {risk_level}/10 (ì‹ ë¢°ë„: {confidence:.2f})")
            return result
            
        except Exception as e:
            logger.error(f"âŒ ì•ˆì „ ë¶„ì„ ì‹¤íŒ¨: {str(e)}")
            # ì—ëŸ¬ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
            return SafetyAnalysisResponse(
                risk_level=5,
                confidence=0.5,
                recommendations=["ì‹œìŠ¤í…œ ì ê²€ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."],
                risk_factors=["ë¶„ì„ ì˜¤ë¥˜"],
                timestamp=datetime.now().isoformat(),
                model_version=self.model_version,
                analysis_details={"error": str(e)}
            )

    def _extract_features(self, request: SafetyAnalysisRequest) -> Dict[str, float]:
        """ìš”ì²­ ë°ì´í„°ì—ì„œ íŠ¹ì„± ì¶”ì¶œ"""
        features = {}
        
        # ì•± ì‚¬ìš© íŠ¹ì„±
        if request.app_usage:
            features["screen_time"] = request.app_usage.screen_time
            features["app_open_count"] = request.app_usage.app_open_count
            
            # ë§ˆì§€ë§‰ í™œë™ìœ¼ë¡œë¶€í„°ì˜ ì‹œê°„
            try:
                last_activity = datetime.fromisoformat(request.app_usage.last_activity.replace('Z', '+00:00'))
                features["hours_since_activity"] = (datetime.now() - last_activity).total_seconds() / 3600
            except:
                features["hours_since_activity"] = 0
        
        # ì²´í¬ì¸ íŠ¹ì„±
        if request.last_checkin:
            try:
                last_checkin = datetime.fromisoformat(request.last_checkin.replace('Z', '+00:00'))
                features["hours_since_checkin"] = (datetime.now() - last_checkin).total_seconds() / 3600
            except:
                features["hours_since_checkin"] = 24  # ê¸°ë³¸ê°’
        else:
            features["hours_since_checkin"] = 24
        
        # ìœ„ì¹˜ íŠ¹ì„± (í–¥í›„ í™•ì¥ ê°€ëŠ¥)
        if request.location:
            features["has_location"] = 1
            # ìœ„ì¹˜ ë³€í™”ëŸ‰ ê³„ì‚° (í–¥í›„ êµ¬í˜„)
            features["location_changes"] = 0
        else:
            features["has_location"] = 0
            features["location_changes"] = 0
        
        # ê¸°ë³¸ê°’ ì„¤ì •
        features.setdefault("screen_time", 0)
        features.setdefault("app_open_count", 0)
        features.setdefault("hours_since_activity", 24)
        features.setdefault("location_changes", 0)
        
        return features

    async def _predict_with_model(self, features: Dict[str, float]) -> tuple:
        """AI ëª¨ë¸ì„ ì‚¬ìš©í•œ ì˜ˆì¸¡"""
        try:
            # íŠ¹ì„± ë²¡í„° ìƒì„±
            feature_vector = np.array([[
                features["screen_time"],
                features["app_open_count"],
                features["hours_since_checkin"],
                features["location_changes"]
            ]])
            
            # ìŠ¤ì¼€ì¼ë§
            feature_scaled = self.scaler.transform(feature_vector)
            
            # ì˜ˆì¸¡
            prediction = self.model.predict(feature_scaled)[0]
            probabilities = self.model.predict_proba(feature_scaled)[0]
            
            # ìœ„í—˜ë„ë¥¼ 0-10 ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
            risk_level = prediction * 3.33  # 0,1,2 -> 0,3.33,6.66
            confidence = np.max(probabilities)
            
            return min(risk_level, 10), confidence
            
        except Exception as e:
            logger.error(f"âŒ AI ëª¨ë¸ ì˜ˆì¸¡ ì‹¤íŒ¨: {str(e)}")
            return self._rule_based_analysis(features)

    def _rule_based_analysis(self, features: Dict[str, float]) -> tuple:
        """ê·œì¹™ ê¸°ë°˜ ì•ˆì „ ë¶„ì„"""
        risk_score = 0
        
        # í™”ë©´ ì‚¬ìš© ì‹œê°„ ë¶„ì„
        if features["screen_time"] < 30:  # 30ë¶„ ë¯¸ë§Œ
            risk_score += 2
        elif features["screen_time"] < 60:  # 1ì‹œê°„ ë¯¸ë§Œ
            risk_score += 1
        
        # ì•± ì‹¤í–‰ íšŸìˆ˜ ë¶„ì„
        if features["app_open_count"] < 3:
            risk_score += 2
        elif features["app_open_count"] < 8:
            risk_score += 1
        
        # ì²´í¬ì¸ ì‹œê°„ ë¶„ì„
        hours_since_checkin = features["hours_since_checkin"]
        if hours_since_checkin > 48:  # 48ì‹œê°„ ì´ìƒ
            risk_score += 4
        elif hours_since_checkin > 24:  # 24ì‹œê°„ ì´ìƒ
            risk_score += 3
        elif hours_since_checkin > 12:  # 12ì‹œê°„ ì´ìƒ
            risk_score += 1
        
        # í™œë™ ì‹œê°„ ë¶„ì„
        hours_since_activity = features["hours_since_activity"]
        if hours_since_activity > 6:
            risk_score += 1
        
        # ìœ„í—˜ë„ë¥¼ 0-10ìœ¼ë¡œ ì •ê·œí™”
        risk_level = min(risk_score, 10)
        confidence = 0.8  # ê·œì¹™ ê¸°ë°˜ì€ ë†’ì€ ì‹ ë¢°ë„
        
        return risk_level, confidence

    def _generate_recommendations(self, risk_level: int, features: Dict[str, float]) -> List[str]:
        """ìœ„í—˜ë„ì— ë”°ë¥¸ ì¶”ì²œì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        if risk_level >= 8:
            recommendations.extend([
                "ğŸš¨ ì¦‰ì‹œ ì•ˆì „ ìƒíƒœë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”",
                "ë¹„ìƒì—°ë½ì²˜ì— ì—°ë½ì„ ê³ ë ¤í•´ë³´ì„¸ìš”",
                "ê°€ê¹Œìš´ ì´ì›ƒì´ë‚˜ ê°€ì¡±ì—ê²Œ ì•ˆë¶€ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”"
            ])
        elif risk_level >= 5:
            recommendations.extend([
                "âš ï¸ ì •ê¸° ì²´í¬ì¸ì„ í•´ì£¼ì„¸ìš”",
                "ì´ì›ƒê³¼ ì†Œí†µí•´ë³´ì„¸ìš”",
                "ì•ˆì „ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•´ì£¼ì„¸ìš”"
            ])
        elif risk_level >= 3:
            recommendations.extend([
                "ğŸ“± ì£¼ê¸°ì ìœ¼ë¡œ ì²´í¬ì¸í•´ì£¼ì„¸ìš”",
                "ì»¤ë®¤ë‹ˆí‹° í™œë™ì— ì°¸ì—¬í•´ë³´ì„¸ìš”"
            ])
        else:
            recommendations.append("ğŸ˜Š ì¢‹ì€ í•˜ë£¨ ë³´ë‚´ì„¸ìš”!")
        
        # íŠ¹ì„±ë³„ ê°œë³„ ì¶”ì²œ
        if features["screen_time"] < 60:
            recommendations.append("ğŸ“± ì•±ì„ ì¡°ê¸ˆ ë” í™œìš©í•´ë³´ì„¸ìš”")
        
        if features["hours_since_checkin"] > 12:
            recommendations.append("â° ì²´í¬ì¸ ì£¼ê¸°ë¥¼ ì¤„ì—¬ë³´ëŠ” ê²ƒì„ ê³ ë ¤í•´ë³´ì„¸ìš”")
        
        return recommendations

    def _identify_risk_factors(self, features: Dict[str, float]) -> List[str]:
        """ìœ„í—˜ ìš”ì†Œ ì‹ë³„"""
        risk_factors = []
        
        if features["screen_time"] < 30:
            risk_factors.append("ë‚®ì€ ì•± ì‚¬ìš©ëŸ‰")
        
        if features["app_open_count"] < 5:
            risk_factors.append("ë‚®ì€ ì•± ì‹¤í–‰ ë¹ˆë„")
        
        if features["hours_since_checkin"] > 24:
            risk_factors.append("ì˜¤ëœ ì²´í¬ì¸ ê³µë°±")
        
        if features["hours_since_activity"] > 6:
            risk_factors.append("ìµœê·¼ í™œë™ ì—†ìŒ")
        
        if not risk_factors:
            risk_factors.append("íŠ¹ë³„í•œ ìœ„í—˜ ìš”ì†Œ ì—†ìŒ")
        
        return risk_factors

    async def learn_user_pattern(self, user_id: str, data: Dict[str, Any]):
        """ì‚¬ìš©ì íŒ¨í„´ í•™ìŠµ (ë°±ê·¸ë¼ìš´ë“œ ì‘ì—…)"""
        try:
            logger.info(f"ğŸ“š ì‚¬ìš©ì {user_id} íŒ¨í„´ í•™ìŠµ ì‹œì‘")
            
            # TODO: ì‹¤ì œ íŒ¨í„´ í•™ìŠµ ë¡œì§ êµ¬í˜„
            # í˜„ì¬ëŠ” ë¡œê·¸ë§Œ ê¸°ë¡
            
            logger.info(f"âœ… ì‚¬ìš©ì {user_id} íŒ¨í„´ í•™ìŠµ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ íŒ¨í„´ í•™ìŠµ ì‹¤íŒ¨: {str(e)}")

    async def get_model_status(self) -> ModelStatus:
        """ëª¨ë¸ ìƒíƒœ ì¡°íšŒ"""
        return ModelStatus(
            model_name="SafetyAnalyzer",
            version=self.model_version,
            status="operational" if self.is_initialized else "initializing",
            last_trained=None,  # TODO: ì‹¤ì œ í•™ìŠµ ì‹œê°„ ê¸°ë¡
            accuracy=0.85 if self.model is not None else None,
            total_predictions=self.performance_metrics["total_predictions"]
        )

    async def get_performance_metrics(self) -> PerformanceMetrics:
        """ì„±ëŠ¥ ì§€í‘œ ì¡°íšŒ"""
        response_times = self.performance_metrics["response_times"]
        
        return PerformanceMetrics(
            accuracy=0.85,  # TODO: ì‹¤ì œ ì •í™•ë„ ê³„ì‚°
            precision=0.82,
            recall=0.88,
            f1_score=0.85,
            total_predictions=self.performance_metrics["total_predictions"],
            avg_response_time=np.mean(response_times) if response_times else 0.0,
            last_updated=datetime.now().isoformat()
        )
